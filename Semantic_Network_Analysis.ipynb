{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Semantic Network Analysis.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lsEQePA3lWx",
        "outputId": "22ab8937-95a5-4c8f-f3a7-f7e66453f992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxvl7Xqq4bbw",
        "outputId": "11c22327-08ad-40c3-9d7c-b20f496db53a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import nltk\n",
        "wn = nltk.WordNetLemmatizer()\n",
        "ps = nltk.PorterStemmer()\n",
        "import glob\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import csv\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "import string\n",
        "import itertools\n",
        "import zipfile\n",
        "import json\n",
        "punctuation = string.punctuation\n",
        "stopwordsset = set(stopwords.words(\"english\"))\n",
        "stopwordsset.add('rt')\n",
        "stopwordsset.add(\"'s\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyNtS2lP6e_q"
      },
      "source": [
        "Clean up our twitter data and gives us a semantic network. We want to figure out what words are the most related. \n",
        "\n",
        "Stemming gets us to the root of the word. \n",
        "\n",
        "You also want to remove stopwods/common words. \n",
        "\n",
        "Lemmitization is taking the root of the word. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tNQRyqP6DdI"
      },
      "source": [
        "#Removing urls\n",
        "def removeURL(text):\n",
        "  result = re.sub(r\"http\\S+\", \"\", text)\n",
        "  return result\n",
        "#Extracting contextual words from a sentence\n",
        "# tokenizing is taking out all the words in a sentence and turning it into tokens/words\n",
        "def tokenize(text):\n",
        "  #lower case\n",
        "  text = text.lower()\n",
        "  #split into individual words\n",
        "  words = word_tokenize(text)\n",
        "  return words\n",
        "#stem - peaches : peach : reduce the number of repeated words\n",
        "def stem(tokenizedtext):\n",
        "  rootwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    aword = ps.stem(aword)\n",
        "    rootwords.append(aword)\n",
        "  return rootwords\n",
        "#removes useless words such as a, an, the\n",
        "def stopWords(tokenizedtext):\n",
        "  goodwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    if aword not in stopwordsset:\n",
        "      goodwords.append(aword)\n",
        "  return goodwords\n",
        "# feature reduction. taking words and getting their roots and graphing only the root words\n",
        "def lemmatizer(tokenizedtext):\n",
        "  lemmawords = []\n",
        "  for aword in tokenizedtext:\n",
        "    aword = wn.lemmatize(aword)\n",
        "    lemmawords.append(aword)\n",
        "  return lemmawords\n",
        "#inputs a list of tokens and returns a list of unpunctuated tokens/words\n",
        "def removePunctuation(tokenizedtext):\n",
        "  nopunctwords = []\n",
        "  for aword in tokenizedtext:\n",
        "    if aword not in punctuation:\n",
        "      nopunctwords.append(aword)\n",
        "  cleanedwords = []\n",
        "  for aword in nopunctwords:\n",
        "    aword = aword.translate(str.maketrans('', '', string.punctuation))\n",
        "    cleanedwords.append(aword)\n",
        "  return cleanedwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA7nHTlM6aiU",
        "outputId": "4b1db4e5-5aaa-4c9d-a26f-14200e1a4ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "tweetzipfiles = glob.glob('/content/drive/My Drive/Colab Notebooks/ERPvendors/Main Folder/*.zip')\n",
        "tweetzipfiles"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Colab Notebooks/ERPvendors/Main Folder/@Oracle.zip',\n",
              " '/content/drive/My Drive/Colab Notebooks/ERPvendors/Main Folder/@salesforce.zip',\n",
              " '/content/drive/My Drive/Colab Notebooks/ERPvendors/Main Folder/@SAP.zip']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyTNXaIgDZT_"
      },
      "source": [
        "Using zipfile to open the path, and make it an actual file object. We are loading in the json object. We need to extract out the text from the tweet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbk1xsn4C262",
        "outputId": "c7306abd-f967-4ad8-ca16-ca3cd7710ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "from time import sleep\n",
        "uniquewords = {}\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    count += 1\n",
        "    if count % 1000 == 0:\n",
        "      print(count)\n",
        "    \n",
        "    text = tweetjson['text']\n",
        "    #natural language processing: clean the tweet\n",
        "    nourlstext = removeURL(text)\n",
        "    tokenizedtext = tokenize(nourlstext)\n",
        "    nostopwordstext = stopWords(tokenizedtext)\n",
        "    lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "    nopuncttext = removePunctuation(lemmatizedtext)\n",
        "    \n",
        "  \n",
        "    for aword in nopuncttext:\n",
        "      if aword in uniquewords:\n",
        "        uniquewords[aword] += 1\n",
        "      if aword not in uniquewords:\n",
        "        uniquewords[aword] = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpIoSx8JcJe"
      },
      "source": [
        "The dictionary can be used to figure out what words you want to use. \n",
        "\n",
        "Helps you narrow down the size of your vocab. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY-RgbMAIG58",
        "outputId": "0abc26b7-5bec-4691-c7ae-e69311d53d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(uniquewords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17729"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev_Ra34ybeRC"
      },
      "source": [
        "These are the words to include in a semantic network analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqfQUXCSJkji"
      },
      "source": [
        "#list of unique words: key-word = number of times the word appears\n",
        "wordstoinclude = set()\n",
        "wordcount = 0\n",
        "for aword in uniquewords:\n",
        "  if uniquewords[aword] > 25:\n",
        "    wordcount += 1\n",
        "    wordstoinclude.add(aword)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t7pWZxMLGPb",
        "outputId": "e78b37fd-e459-40c8-b47e-dd7fa4cbcce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print (wordcount)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKqdMS8OcDLn"
      },
      "source": [
        "We want gephi to know that this is an undirected network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARIyO9TiM6W-",
        "outputId": "d00e7a2d-b2ca-473b-d8ae-3cf5efb6bdb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        }
      },
      "source": [
        "edgelist1 = open('/content/drive/My Drive/Colab Notebooks/ERPvendors/Main Folder/gephi-semantic.csv','w')\n",
        "csvwriter = csv.writer(edgelist1)\n",
        "\n",
        "header = ['Source','Target','Type']\n",
        "csvwriter.writerow(header) #write first row with header\n",
        "\n",
        "print ('Writing Edge List')\n",
        "\n",
        "uniquewords = {}\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    count += 1\n",
        "    if count % 1000 == 0:\n",
        "      print(count)\n",
        "\n",
        "    text = tweetjson['text']\n",
        "\n",
        "    nourlstext = removeURL(text)\n",
        "    tokenizedtext = tokenize(nourlstext)\n",
        "    nostopwordstext = stopWords(tokenizedtext)\n",
        "    lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "    nopuncttext = removePunctuation(lemmatizedtext)\n",
        "\n",
        "    goodwords = []\n",
        "    for aword in nopuncttext:\n",
        "      if aword in wordstoinclude:\n",
        "        goodwords.append(aword.replace(',',''))\n",
        "    \n",
        "    allcombos = itertools.combinations(goodwords,2)\n",
        "    for acombo in allcombos:\n",
        "        row = []\n",
        "        for anode in acombo:\n",
        "            row.append(anode)\n",
        "        row.append('Undirected')\n",
        "        csvwriter.writerow(row)\n",
        "\n",
        "edgelist1.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing Edge List\n",
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gASYSXivPOsg",
        "outputId": "eb101686-d65b-493e-8ae3-9c489f9b6cbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "uniquewords = {}\n",
        "count = 0\n",
        "\n",
        "for tweetzipfile in tweetzipfiles:\n",
        "  zf = zipfile.ZipFile(tweetzipfile)\n",
        "  for i, obj in enumerate(zf.infolist()):\n",
        "    tweetjson = json.load(zf.open(obj))\n",
        "    count += 1\n",
        "    if count % 1000 == 0:\n",
        "      print(count)\n",
        "\n",
        "    text = tweetjson['text']\n",
        "\n",
        "    nourlstext = removeURL(text)\n",
        "    tokenizedtext = tokenize(nourlstext)\n",
        "    nostopwordstext = stopWords(tokenizedtext)\n",
        "    lemmatizedtext = lemmatizer(nostopwordstext)\n",
        "    nopuncttext = removePunctuation(lemmatizedtext)\n",
        "\n",
        "    goodwords = []\n",
        "    for aword in nopuncttext:\n",
        "      if aword in wordstoinclude:\n",
        "        goodwords.append(aword.replace(',',''))\n",
        "    \n",
        "    allcombos = itertools.combinations(goodwords,2)\n",
        "    for acombo in allcombos:\n",
        "        row = []\n",
        "        for anode in acombo:\n",
        "            row.append(anode)\n",
        "        row.append('Undirected')\n",
        "        csvwriter.writerow(row)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-726f6065f1c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Undirected'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mcsvwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: I/O operation on closed file."
          ]
        }
      ]
    }
  ]
}